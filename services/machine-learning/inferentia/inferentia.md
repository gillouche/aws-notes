# AWS Inferentia

## General info

machine learning inference chip that supports TensorFlow, Apache MXNet, PyTorch, ONNX models format

used when it is needed to have access to an entire GPU or low latency requirements 

hundreds of TOPS (tera operations per second)

multiple Inferentia chips can be used together

can be used with SageMaker, EC2 and Elastic Inference
